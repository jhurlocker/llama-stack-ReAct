# Default values for llama4-scout
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  name: ""

# Model configuration
model:
  maxModelLen: 8192


# Resource requirements (adjust based on GPU availability)
resources:
  limits:
    nvidia.com/gpu: 4
    memory: 24Gi
    cpu: 4
  requests:
    nvidia.com/gpu: 4
    memory: 16Gi
    cpu: 2


nodeSelector:
  nvidia.com/gpu.present: "true"

tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    value: "NVIDIA-L40S-SHARED"

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: nvidia.com/gpu.present
          operator: In
          values:
          - "true"


# Environment variables
env:
  CUDA_VISIBLE_DEVICES: "0"
  TRANSFORMERS_CACHE: "/root/.cache/huggingface"
  HF_HOME: "/root/.cache/huggingface"

# InferenceService configuration
inferenceService:
  displayName: "llama4-scout"
  maxReplicas: 1
  minReplicas: 1
  modelFormat: "vLLM"
  modelName: ""
  storageUri: "oci://registry.redhat.io/rhelai1/modelcar-llama-4-scout-17b-16e-instruct-quantized-w4a16:1.5"

# Serving Runtime configuration
servingRuntime:
  enabled: true  # Set to true to create a ServingRuntime resource
  image: "registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1"
  shmSizeLimit: "8Gi"
  memBufferBytes: 134217728  # 128MB
  modelLoadingTimeoutMillis: 90000  # 90 seconds